"""Register an Iceberg table in the AWS Glue Data Catalog.

The script reads the Iceberg metadata JSON that Spark produced in this repo to
infer the table schema and partitions, then calls AWS Glue to create (or
optionally update) the matching catalog table. The actual table data and
metadata must already live in Amazon S3 before you run this utility.
"""

from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Any, Dict, List, Optional

import boto3
from botocore.exceptions import ClientError


ICEBERG_TO_GLUE_TYPES = {
    "string": "string",
    "int": "int",
    "integer": "int",
    "long": "bigint",
    "float": "float",
    "double": "double",
    "boolean": "boolean",
    "date": "date",
    "timestamp": "timestamp",
    "timestamptz": "timestamp",
}


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Register an Iceberg table in AWS Glue.")
    parser.add_argument(
        "--database",
        default="iceberg_demo",
        help="Glue database name to create or reuse.",
    )
    parser.add_argument(
        "--table",
        default="order_history",
        help="Glue table name to create or update.",
    )
    parser.add_argument(
        "--metadata-json",
        type=Path,
        default=Path("warehouse/order_history/metadata/v4.metadata.json"),
        help="Path to the Iceberg metadata JSON generated by Spark.",
    )
    parser.add_argument(
        "--table-location",
        required=True,
        help="S3 URI pointing to the Iceberg table root (e.g. s3://bucket/warehouse/order_history).",
    )
    parser.add_argument(
        "--metadata-location",
        help="S3 URI for the Iceberg metadata JSON. Defaults to <table-location>/metadata/<metadata-json filename>.",
    )
    parser.add_argument(
        "--database-location",
        help="Optional S3 URI to store on the Glue database. Defaults to the table location.",
    )
    parser.add_argument(
        "--catalog-id",
        help="Optional Glue catalog ID if you need to target a different AWS account.",
    )
    parser.add_argument(
        "--profile",
        help="boto3 profile name (overrides AWS_PROFILE).",
    )
    parser.add_argument(
        "--region",
        help="AWS region (overrides AWS_REGION).",
    )
    parser.add_argument(
        "--replace",
        action="store_true",
        help="Overwrite the Glue table definition if it already exists.",
    )
    return parser.parse_args()


def normalize_s3_uri(uri: str) -> str:
    if not uri.startswith("s3://"):
        raise ValueError(f"S3 URI must start with s3://, got: {uri}")
    return uri.rstrip("/")


def load_iceberg_layout(metadata_path: Path) -> tuple[List[Dict[str, str]], List[Dict[str, str]]]:
    if not metadata_path.is_file():
        raise FileNotFoundError(f"Metadata file not found: {metadata_path}")

    metadata = json.loads(metadata_path.read_text())
    schema_id = metadata["current-schema-id"]
    schemas = {schema["schema-id"]: schema for schema in metadata["schemas"]}
    schema = schemas.get(schema_id)
    if schema is None:
        raise ValueError(f"Schema id {schema_id} not present in {metadata_path}")

    columns = [
        {"Name": field["name"], "Type": iceberg_to_glue_type(field["type"])}
        for field in schema["fields"]
    ]

    spec_id = metadata.get("default-spec-id")
    spec_lookup = {spec["spec-id"]: spec for spec in metadata.get("partition-specs", [])}
    partition_spec = spec_lookup.get(spec_id, {"fields": []})
    partitions = []
    for field in partition_spec.get("fields", []):
        source = next(
            (candidate for candidate in schema["fields"] if candidate["id"] == field["source-id"]),
            None,
        )
        if source is None:
            raise ValueError(f"Source field id {field['source-id']} missing in schema")
        partitions.append({"Name": source["name"], "Type": iceberg_to_glue_type(source["type"])})

    return columns, partitions


def iceberg_to_glue_type(iceberg_type: str) -> str:
    lower = iceberg_type.lower()
    if lower in ICEBERG_TO_GLUE_TYPES:
        return ICEBERG_TO_GLUE_TYPES[lower]
    if lower.startswith("decimal("):
        return lower
    raise ValueError(f"Unsupported Iceberg type '{iceberg_type}' in metadata")


def ensure_database(
    glue_client: Any,
    catalog_id: Optional[str],
    database_name: str,
    location_uri: str,
) -> None:
    get_args: Dict[str, Any] = {"Name": database_name}
    if catalog_id:
        get_args["CatalogId"] = catalog_id

    try:
        glue_client.get_database(**get_args)
        return
    except glue_client.exceptions.EntityNotFoundException:
        pass

    db_input: Dict[str, Any] = {
        "Name": database_name,
        "Description": "Iceberg demo tables registered from the test-iceberg project.",
    }
    if location_uri:
        db_input["LocationUri"] = location_uri

    create_args: Dict[str, Any] = {"DatabaseInput": db_input}
    if catalog_id:
        create_args["CatalogId"] = catalog_id
    glue_client.create_database(**create_args)


def upsert_table(
    glue_client: Any,
    *,
    catalog_id: Optional[str],
    database_name: str,
    table_name: str,
    table_location: str,
    metadata_location: str,
    columns: List[Dict[str, str]],
    partitions: List[Dict[str, str]],
    replace: bool,
) -> None:
    table_input: Dict[str, Any] = {
        "Name": table_name,
        "TableType": "EXTERNAL_TABLE",
        "Parameters": {
            "EXTERNAL": "TRUE",
            "table_type": "ICEBERG",
            "metadata_location": metadata_location,
            "format": "iceberg/parquet",
            "engine.type": "spark",
        },
        "PartitionKeys": partitions,
        "StorageDescriptor": {
            "Columns": columns,
            "Location": table_location,
            "InputFormat": "org.apache.iceberg.mr.hive.HiveIcebergInputFormat",
            "OutputFormat": "org.apache.iceberg.mr.hive.HiveIcebergOutputFormat",
            "SerdeInfo": {
                "SerializationLibrary": "org.apache.iceberg.mr.hive.HiveIcebergSerDe",
                "Parameters": {"serialization.format": "1"},
            },
        },
    }

    create_args: Dict[str, Any] = {
        "DatabaseName": database_name,
        "TableInput": table_input,
    }
    if catalog_id:
        create_args["CatalogId"] = catalog_id

    try:
        glue_client.create_table(**create_args)
        print(f"Created Glue table {database_name}.{table_name}")
        return
    except glue_client.exceptions.AlreadyExistsException:
        if not replace:
            raise

    update_args = dict(create_args)
    glue_client.update_table(**update_args)
    print(f"Updated Glue table {database_name}.{table_name}")


def main() -> None:
    args = parse_args()
    metadata_json = args.metadata_json
    columns, partitions = load_iceberg_layout(metadata_json)

    table_location = normalize_s3_uri(args.table_location)
    metadata_location = normalize_s3_uri(
        args.metadata_location
        if args.metadata_location
        else f"{table_location}/metadata/{metadata_json.name}"
    )
    database_location = normalize_s3_uri(args.database_location) if args.database_location else table_location

    session_kwargs: Dict[str, str] = {}
    if args.profile:
        session_kwargs["profile_name"] = args.profile
    if args.region:
        session_kwargs["region_name"] = args.region
    session = boto3.session.Session(**session_kwargs)
    glue = session.client("glue")

    ensure_database(glue, args.catalog_id, args.database, database_location)
    upsert_table(
        glue,
        catalog_id=args.catalog_id,
        database_name=args.database,
        table_name=args.table,
        table_location=table_location,
        metadata_location=metadata_location,
        columns=columns,
        partitions=partitions,
        replace=args.replace,
    )


if __name__ == "__main__":
    try:
        main()
    except (ClientError, ValueError, FileNotFoundError) as exc:
        raise SystemExit(f"Failed to register Glue table: {exc}") from exc
